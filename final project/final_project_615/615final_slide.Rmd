---
title: "Yelp Business Analyze"
author: "xiaofei Wu"
date: "2019/12/17"
output: ioslides_presentation
---

```{r setup, include=FALSE,warning=FALSE, message=FALSE,,echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages,warning=FALSE, message=FALSE,echo=FALSE}
# install.packages("rjson")
# Load the package required to read JSON files.
library("rjson")
library(jsonlite)

require(maps)
require(maptools)
require(tidyverse)
require(tidyr)
require(magrittr)
library(usmap)
library(ggplot2)
library(ggrepel)
library(wordcloud)
library(tm)
library(knitr)
library(tidytext)
```

```{r load data,warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}
# Load data business.
business <- stream_in(file("business.json"))
review_sample<-stream_in(file("review_sample1.csv"))

#delete all state that's not in r using subset function
state<- subset(business, state!="BAS"&state!="CON"&state!="DOW"&state!="DUR"&state!="XGM"&state!= "XWY"&state!= "XGL"&state!= "AB"&state!= "BC"&state!= "ON"&state!= "QC")
```

## Table of Content


- Project Overview
- Data Manipulation
- Maps: Method
- Maps: Examples & Results
- WordClouds: Method
- WordClouds: Examples & Results
- SENTIMENTAL ANALYZE
- Discussions & Limitation
- Shiny & Future Work

## Yelp Project Overview 

This is an Yelp Business Analyze. This presentation shares discoveries from conducting research and analysis on Yelp Dataset. For more details on data source see <https://www.yelp.com/dataset/download>.

This research forcuses on dataset 'business' for restaurant informations and dataset 'review' for clients' comment towards these restanrant. Distribution of the yelp business will be showed in three different dimensions: Worldwide, US, States. This research will also show the features of the most popular restaurants through WordClouds. 


## Data Manipulation 
- The json file of dataset from yelp was read in and expanded columns using 'stream_in' from package 'jsonlite'. 
- For state maps, I deleted all state that's not in r using 'subset' function. 
- For WordClouds, I did text mining to match the argument pattern, using functions as 'gsub', 'tolower', 'strsplit'. 


## Maps: Method
- Plot maps using 'plot_usmap' and 'geom_point'
- To be more fair, I delete the state with one or two business when counting the review density and star rate. 
- I leave buisness with five or more comments when calculating the overall correlations. 

## Maps: Examples & Results
### World Map
```{r fig.width=8,fig.height=4, warning=FALSE, message=FALSE, echo=FALSE}
# create a layer of borders
## World Map of Yelp Business
mapWorld <- borders("world", colour="white", fill="lightblue") 
ggplot() + mapWorld +
  theme(axis.line = element_blank(), 
        axis.text = element_blank(), 
        axis.ticks = element_blank(), 
        axis.title = element_blank(), 
        plot.background = element_blank()) +
  geom_point(data = business, 
             aes(x = longitude, y = latitude, color = stars), 
             size = 2, 
             alpha = 0.6) +
  ggtitle('Figure 1: World Map of Yelp Business') +
  theme(legend.title = element_text(size = 10), 
        legend.text = element_text(size = 10),
        legend.justification=c(0,0), legend.position=c(0,0.3))
```

## Maps: Examples & Results
### US Map
```{r fig.width=8,fig.height=4, warning=FALSE, message=FALSE, echo=FALSE}
# create a layer of borders
mapUSA <- borders("usa", colour="white", fill="lightblue") 
ggplot() + mapUSA +
  theme(axis.line = element_blank(), 
        axis.text = element_blank(), 
        axis.ticks = element_blank(), 
        axis.title = element_blank(), 
        plot.background = element_blank()) +
  geom_point(data = business, 
             aes(x = longitude, y = latitude, color = stars), 
             size = 5, 
             alpha = 0.6) +
  ggtitle('Figure 2: USA Map of Yelp Business') +
  theme(legend.title = element_text(size = 10), 
        legend.text = element_text(size = 10),
        legend.justification=c(0,0), legend.position=c(0,0.3))
```

## Maps: Examples & Results
### US Map(Devided by states)
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# create a layer of borders
mapstates <- borders("state", colour="white", fill="lightblue") 
ggplot() + mapstates +
  theme(axis.line = element_blank(), 
        axis.text = element_blank(), 
        axis.ticks = element_blank(), 
        axis.title = element_blank(), 
        plot.background = element_blank()) +
  geom_point(data = business, 
             aes(x = longitude, y = latitude), 
             size = .5, 
             alpha = 0.6) +
  ggtitle('Figure 3: USA Map of Yelp Business Devided by states') +
  theme(legend.title = element_text(size = 10), 
        legend.text = element_text(size = 10),
        legend.justification=c(0,0), legend.position=c(0,0.3)) 

```

## Maps: Examples & Results
### State Maps 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# count the number of business in different states
statemap <- state %>%
  group_by(state) %>%
  count(state)%>%
  rename(n_business = n)
# creat a STATE map for Yelp Business Density in US
plot_usmap(regions = "states", data = statemap, values = "n_business")+scale_fill_continuous(low = "white", high = "orange",name = "Figure 4: Yelp Business Density in states", label = scales::comma) + 
  theme(legend.position = "right")
```

## Maps: Examples & Results
### Review Density
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# count the number of reviews in different states
reviewmap <- state %>%
  group_by(state) %>%
  count(review_count)%>%
  mutate(nreview = review_count*n)

reviewn<-aggregate(reviewmap$nreview, by=list(Category=reviewmap$state), FUN=sum)%>%
  rename(state = Category)%>%
  rename(n_reviews = x)
reviewmap <- merge(statemap,reviewn,by="state")%>%
  mutate(review_density = n_reviews/n_business)
##### The review numbers seems to be propotional to number of business
 
# creat a US map for Yelp Business Reviews Density in US
# to be more fair, delete the state with only one or two business
reviewmap_2<-filter(reviewmap,n_business>2)
plot_usmap(regions = "states", data = reviewmap_2, values = "review_density")+scale_fill_continuous(low = "white", high = "brown",name = "Figure 5: Yelp Business Review Density in US", label = scales::comma) + theme(legend.position = "right")
```

## Maps: Examples & Results
### Star Rate 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# count the number of stars in different states
starmap <- state %>%
  group_by(state) %>%
  count(stars)%>%
  mutate(nstars = stars*n)
star<-aggregate(starmap$nstars, by=list(Category=starmap$state), FUN=sum)%>%
  rename(state = Category)%>%
  rename(sum_star = x)
starmap <- merge(star,statemap,by="state")%>%
  mutate(mean_star = sum_star/n_business)

# to be more fair, delete the state with one or two business
starmap_2<-filter(starmap,n_business>2)
# creat a US map for Yelp Business Reviews Density in US
plot_usmap(regions = "states", data = starmap_2, values = "mean_star")+scale_fill_continuous(low = "white", high = "red",name = "Figure 5: Yelp Business Average Rate Star in US", label = scales::comma) + theme(legend.position = "right")
```



## Maps: Examples & Results
### Aggerate Correlation 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
sum1<-merge(reviewmap,starmap,by = "state")

# leave buisness with five or more comments
sum2<-filter(sum1,sum1$n_business.x>4)
cor2<-cor(sum2[,c(2,3,4,7)])
as.tibble(cor2)
# cor between mean_star and rate density is 0.69138460
### RESULT : State with more reviews have better overall rate. 
# prople are more likely to leave a comment when they like their meal.  
```


## Maps: Examples & Results

- The review density do not follow the number of business. People from some state are more tend to leave some comments.
- Correlation between mean_star and rate density is 0.69138460. State with more reviews density has better overall rate.


## WordClouds: Method

- Text Analysis
- Text Tidy: using functions as 'removeWords', 'gsub', 'tolower', 'strsplit'
- I create a function that break customer reviews in words and analyze word counts that contribute to each sentiment

## WordClouds: Examples & Results
### Take a look at the top 10 frequent words. 
```{r,warning=FALSE, echo=FALSE}
review<-review_sample
# convert all text to lower case
review$text <- tolower(review$text)
# remove punctuation
review$text <- gsub("[[:punct:]]", "", review$text)
# remove punctuation
review$text <- gsub("\\n", "", review$text)
# split sentences into words. 
review$text<- strsplit(review$text, " ")

text<-unlist(review$text)

# Remove stop-words
text = removeWords(text, c(stopwords("english"),"end","get","say","years","come","going","2012"))
text = text[c(1:100000)]
# take a look at the top 10 frequent words. 
s1<-sort(table(text), decreasing=T)[1:10]
s1
```

## WordClouds: Examples & Results
### Overall Review Cloud
```{r,warning=FALSE, message=FALSE, echo=FALSE}
docs <- Corpus(VectorSource(text))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

# Make cloud

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

## WordClouds: Examples & Results
### Top1 Popular Restaurant Review Cloud
```{r,warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}
# count the number of reviews in different business
group_review <- review %>%
  group_by(business_id) %>%
  count(business_id)
# get the top 5 buisness with the most reviews 
top_review<-group_review[order(-group_review$n),]
 top_review<- top_review[c(1:5),]
 
star_b<-aggregate(review$stars, by=list(Category=review$business_id), FUN=sum)%>%
  rename(business_id = Category)%>%
  rename(sum_star = x)

#combine stars
top_review<-merge(top_review,star_b,by = "business_id")
top_review%>%mutate(average_star=sum_star/n)
```

```{r,warning=FALSE, message=FALSE, echo=FALSE}
## restaurant top 1
b1<-filter(review, business_id == top_review[1,1])

text1<-unlist(b1$text)
# Remove stop-words
text1 = removeWords(text1, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text1, file = "text1.Rdata")
text1 <- readRDS('text1.Rdata')

docs1 <- Corpus(VectorSource(text1))
dtm1 <- TermDocumentMatrix(docs1)
m1 <- as.matrix(dtm1)
v1 <- sort(rowSums(m1),decreasing=TRUE)
d1 <- data.frame(word = names(v1),freq=v1)
# Make cloud
set.seed(1234)
wordcloud(words = d1$word, freq = d1$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

## WordClouds: Examples & Results
### Top2 Popular Restaurant Review Cloud
```{r,warning=FALSE, message=FALSE, echo=FALSE}
## restaurant top 2
b2<-filter(review, business_id == top_review[2,1])
text2<-unlist(b2$text)
# Remove stop-words
text2 = removeWords(text2, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text2, file = "text2.Rdata")
text2 <- readRDS('text2.Rdata')

docs2 <- Corpus(VectorSource(text2))
dtm2 <- TermDocumentMatrix(docs2)
m2 <- as.matrix(dtm2)
v2 <- sort(rowSums(m2),decreasing=TRUE)
d2 <- data.frame(word = names(v2),freq=v2)
# Make cloud
set.seed(1234)
wordcloud(words = d2$word, freq = d2$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

## WordClouds: Examples & Results
### Top3 Popular Restaurant Review Cloud

```{r,warning=FALSE, message=FALSE, echo=FALSE}
b3<-filter(review, business_id == top_review[3,1])
text3<-unlist(b3$text)
# Remove stop-words
text3 = removeWords(text3, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text3, file = "text3.Rdata")
text3 <- readRDS('text3.Rdata')

text3 = text3[c(1:1000)]
docs3 <- Corpus(VectorSource(text3))
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
# Make cloud
set.seed(1234)
wordcloud(words = d3$word, freq = d3$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

## WordClouds: Examples & Results
### Top4 Popular Restaurant Review Cloud

```{r,warning=FALSE, message=FALSE, echo=FALSE}
## restaurant top 4
b4<-filter(review, business_id == top_review[4,1])
text4<-unlist(b4$text)
# Remove stop-words
text4 = removeWords(text4, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text4, file = "text4.Rdata")
text4 <- readRDS('text4.Rdata')

text4 = text4[c(1:1000)]
docs4 <- Corpus(VectorSource(text4))
dtm4 <- TermDocumentMatrix(docs4)
m4 <- as.matrix(dtm4)
v4 <- sort(rowSums(m4),decreasing=TRUE)
d4 <- data.frame(word = names(v4),freq=v4)
# Make cloud
set.seed(1234)
wordcloud(words = d4$word, freq = d4$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

## WordClouds: Examples & Results
### Top5 Popular Restaurant Review Cloud

```{r,warning=FALSE, message=FALSE, echo=FALSE}
b5<-filter(review, business_id == top_review[5,1])
text5<-unlist(b5$text)
# Remove stop-words
text5 = removeWords(text5, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text5, file = "text5.Rdata")
text5 <- readRDS('text5.Rdata')

text5 = text5[c(1:1000)]
docs5 <- Corpus(VectorSource(text5))
dtm5 <- TermDocumentMatrix(docs5)
m5 <- as.matrix(dtm5)
v5 <- sort(rowSums(m5),decreasing=TRUE)
d5 <- data.frame(word = names(v5),freq=v5)
# Make cloud
set.seed(1234)
wordcloud(words = d5$word, freq = d5$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

## SENTIMENTAL ANALYZE 
### positive

```{r,warning=FALSE, echo=FALSE}
# Create a function that break customer reviews in words and analyze word counts that contribute to each sentiment
sentiment<-merge(sentiments,d,by ='word')%>% group_by(sentiment) %>%
    top_n(10)

positive<-sentiment%>%filter(sentiment == "positive")
negative<-sentiment%>%filter(sentiment == "negative")
positive
```

## SENTIMENTAL ANALYZE 
### negative

```{r}
negative
```

## Discussions & Limitation

- Discussions: The wordcloud is very straight forward that it throws a bunch of common features to users about the restaurant. I enconter some problems

- Limitation: The reviews dataset is 5GB which is too big to readin, I randomly sampled 200000 rows to use. So it may not contains all information and has some bias. 


## Shiny & Future Work

- Thanks to <https://shiny.rstudio.com/gallery/> for sources about shinyapp.
- I public my shinyapp as an interactive platform to share my analyze and result. It can be found as <https://vivianwu.shinyapps.io/final615/>
- Due to time concerns, I did not do further analysis, But the next step could be conbining user dataset with the business dataset and review dataset, to get an idea about the different preference of food from clients in different area. 

