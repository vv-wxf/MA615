---
title: "615_final"
author: "xiaofei_wu"
date: "12/15/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r packages}
# install.packages("rjson")
# Load the package required to read JSON files.
library("rjson")
library(jsonlite)

require(maps)
require(maptools)
require(tidyverse)
require(tidyr)
require(magrittr)
library(usmap)
library(ggplot2)
library(ggrepel)
library(wordcloud)
library(tm)
library(knitr)
library(tidytext)
library(png)
library(imager)
```

```{r load data, echo=FALSE}
# Load data business.
business <- stream_in(file("business.json"))
review_sample<-stream_in(file("review_sample1.csv"))
user_sample<-stream_in(file("final_project_615/yelp_dataset/user_sample1.csv"))
# Print the result.
print(business)
# see all columns
colnames(business)

#delete all state that's not in r using subset function
state<- subset(business, state!="BAS"&state!="CON"&state!="DOW"&state!="DUR"&state!="XGM"&state!= "XWY"&state!= "XGL"&state!= "AB"&state!= "BC"&state!= "ON"&state!= "QC")
```



## World Map of Yelp Business
```{r fig.width=12,fig.height=6, warning=FALSE, message=FALSE, echo=FALSE}
# create a layer of borders
mapWorld <- borders("world", colour="white", fill="lightblue") 
ggplot() + mapWorld +
  theme(axis.line = element_blank(), 
        axis.text = element_blank(), 
        axis.ticks = element_blank(), 
        axis.title = element_blank(), 
        plot.background = element_blank()) +
  geom_point(data = business, 
             aes(x = longitude, y = latitude, color = stars), 
             size = 2, 
             alpha = 0.6)+
  ggtitle('Figure 1: World Map of Yelp Business') +
  theme(legend.title = element_text(size = 10), 
        legend.text = element_text(size = 10),
        legend.justification=c(0,0), legend.position=c(0,0.3))

ggsave("businessWorld.png")
```

## US Map of Yelp Business
```{r fig.width=12,fig.height=6, warning=FALSE, message=FALSE, echo=FALSE}
# create a layer of borders
mapUSA <- borders("usa", colour="white", fill="lightblue") 
ggplot() + mapUSA +
  theme(axis.line = element_blank(), 
        axis.text = element_blank(), 
        axis.ticks = element_blank(), 
        axis.title = element_blank(), 
        plot.background = element_blank()) +
  geom_point(data = business, 
             aes(x = longitude, y = latitude, color = stars), 
             size = 5, 
             alpha = 0.6) +
  ggtitle('Figure 2: USA Map of Yelp Business') +
  theme(legend.title = element_text(size = 10), 
        legend.text = element_text(size = 10),
        legend.justification=c(0,0), legend.position=c(0,0.3))

ggsave("businessUS.png")
```

## State Maps

## Business Density
```{r}
# count the number of business in different states
statemap <- state %>%
  group_by(state) %>%
  count(state)%>%
  rename(n_business = n)
# check all observations are counted
sum(statemap$n_business)==nrow(state) #TRUE
statemap$state

# creat a STATE map for Yelp Business Density in US
plot_usmap(regions = "states", data = statemap, values = "n_business")+scale_fill_continuous(low = "white", high = "orange",name = "Figure 3: Yelp Business Density in states", label = scales::comma) + 
  theme(legend.position = "right")


# create a layer of borders
mapstates <- borders("state", colour="white", fill="lightblue") 
ggplot() + mapstates +
  theme(axis.line = element_blank(), 
        axis.text = element_blank(), 
        axis.ticks = element_blank(), 
        axis.title = element_blank(), 
        plot.background = element_blank()) +
  geom_point(data = business, 
             aes(x = longitude, y = latitude), 
             size = .5, 
             alpha = 0.6) +
  ggtitle('Figure 3: USA Map of Yelp Business Devided by states') +
  theme(legend.title = element_text(size = 10), 
        legend.text = element_text(size = 10),
        legend.justification=c(0,0), legend.position=c(0,0.3)) 

ggsave("distribution.png")
img <- readPNG("distribution.png")
print(img)
plot(img)
# count the number of city in different states
citymap <- state %>%
  group_by(city) %>%
  count(city)%>%
  rename(n_business = n)
# check all observations are counted
sum(statemap$n_business)==nrow(state) #TRUE
citymap$city

# creat a US map for Yelp Business Density in US

```
## Review Density
```{r}
# count the number of reviews in different states
reviewmap <- state %>%
  group_by(state) %>%
  count(review_count)%>%
  mutate(nreview = review_count*n)

reviewn<-aggregate(reviewmap$nreview, by=list(Category=reviewmap$state), FUN=sum)%>%
  rename(state = Category)%>%
  rename(n_reviews = x)
reviewmap <- merge(statemap,reviewn,by="state")%>%
  mutate(review_density = n_reviews/n_business)

# # creat a US map for Yelp Business Reviews Number in US
# plot_usmap(regions = "states", data = reviewmap, values = "n_reviews")+scale_fill_continuous(low = "white", high = "brown",name = "Figure 4: Yelp Business Review Numbers in US", label = scales::comma) + 
#   theme(legend.position = "right")
##### The review numbers seems to be propotional to number of business
 


# creat a US map for Yelp Business Reviews Density in US
# to be more fair, delete the state with only one or two business
reviewmap_2<-filter(reviewmap,n_business>2)

plot_usmap(regions = "states", data = reviewmap_2, values = "review_density")+scale_fill_continuous(low = "white", high = "brown",name = "Figure 4: Yelp Business Review Density in US", label = scales::comma) + theme(legend.position = "right")

ggsave("density.png")
##### The review density do not follow the number of business. 
##### People from some state are more tend to leave some comments. 
```
## Star Rate 
```{r}
# count the number of stars in different states
starmap <- state %>%
  group_by(state) %>%
  count(stars)%>%
  mutate(nstars = stars*n)
star<-aggregate(starmap$nstars, by=list(Category=starmap$state), FUN=sum)%>%
  rename(state = Category)%>%
  rename(sum_star = x)

starmap <- merge(star,statemap,by="state")%>%
  mutate(mean_star = sum_star/n_business)

# to be more fair, delete the state with one or two business
starmap_2<-filter(starmap,n_business>2)
# creat a US map for Yelp Business Reviews Density in US
plot_usmap(regions = "states", data = starmap_2, values = "mean_star")+scale_fill_continuous(low = "white", high = "red",name = "Figure 5: Yelp Business Average Rate Star in US", label = scales::comma) + theme(legend.position = "right")

ggsave("average.png")
```

## aggerate correlation 
```{r}
sum1<-merge(reviewmap,starmap,by = "state")
cor1<-cor(sum1[,c(2,3,4,7)])
as.tibble(cor1)

# leave buisness with five or more comments
sum2<-filter(sum1,sum1$n_business.x>4)
cor2<-cor(sum2[,c(2,3,4,7)])
as.tibble(cor2)
# cor between mean_star and rate density is 0.69138460
### RESULT : State with more reviews have better overall rate. 
# prople are more likely to leave a comment when they like their meal.  
```

## user
```{r}

```

## review

## review cloud
```{r,warning=false}

review<-review_sample
# convert all text to lower case
review$text <- tolower(review$text)
# remove punctuation
review$text <- gsub("[[:punct:]]", "", review$text)
# remove punctuation
review$text <- gsub("\\n", "", review$text)
# split sentences into words. 
review$text<- strsplit(review$text, " ")




text<-unlist(review$text)

# Remove stop-words
text = removeWords(text, c(stopwords("english"),"end","get","say","years","come","going","2012"))
text = text[c(1:100000)]
# take a look at the top 100 frequent words. 
sort(table(text), decreasing=T)[1:100]

docs <- Corpus(VectorSource(text))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

# Make cloud

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

## SENTIMENTAL ANALYZE
```{r warning=FALSE, message=FALSE, echo=FALSE}
# Create a function that break customer reviews in words and analyze word counts that contribute to each sentiment
sentiment<-merge(sentiments,d,by ='word')%>% group_by(sentiment) %>%
    top_n(10)

positive<-kable(sentiment%>%filter(sentiment == "positive"), format = "latex", booktabs = TRUE)
negative<-kable(sentiment%>%filter(sentiment == "negative"), format = "latex", booktabs = TRUE)
positive
negative
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
# count the number of reviews in different business
group_review <- review %>%
  group_by(business_id) %>%
  count(business_id)
# get the top 5 buisness with the most reviews 
top_review<-group_review[order(-group_review$n),]
 top_review<- top_review[c(1:5),]
 
star_b<-aggregate(review$stars, by=list(Category=review$business_id), FUN=sum)%>%
  rename(business_id = Category)%>%
  rename(sum_star = x)

#combine stars
top_review<-merge(top_review,star_b,by = "business_id")
top_review%>%mutate(average_star=sum_star/n)
```
```{r,warning=false}
## restaurant top 1
b1<-filter(review, business_id == top_review[1,1])

text1<-unlist(b1$text)
# Remove stop-words
text1 = removeWords(text1, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text1, file = "text1.Rdata")
text1 <- readRDS('text1.Rdata')


# take a look at the top 10 frequent words. 
sort(table(text1), decreasing=T)[1:10]
docs1 <- Corpus(VectorSource(text1))
dtm1 <- TermDocumentMatrix(docs1)
m1 <- as.matrix(dtm1)
v1 <- sort(rowSums(m1),decreasing=TRUE)
d1 <- data.frame(word = names(v1),freq=v1)
# Make cloud
set.seed(1234)
wordcloud(words = d1$word, freq = d1$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


## restaurant top 2
b2<-filter(review, business_id == top_review[2,1])
text2<-unlist(b2$text)
# Remove stop-words
text2 = removeWords(text2, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text2, file = "text2.Rdata")
text2 <- readRDS('text2.Rdata')
# take a look at the top 10 frequent words. 
sort(table(text2), decreasing=T)[1:10]
docs2 <- Corpus(VectorSource(text2))
dtm2 <- TermDocumentMatrix(docs2)
m2 <- as.matrix(dtm2)
v2 <- sort(rowSums(m2),decreasing=TRUE)
d2 <- data.frame(word = names(v2),freq=v2)
# Make cloud
set.seed(1234)
wordcloud(words = d2$word, freq = d2$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

## restaurant top 3
b3<-filter(review, business_id == top_review[3,1])
text3<-unlist(b3$text)
# Remove stop-words
text3 = removeWords(text3, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text3, file = "text3.Rdata")
text3 <- readRDS('text3.Rdata')

# take a look at the top 10 frequent words. 
sort(table(text3), decreasing=T)[1:10]
text3 = text3[c(1:1000)]
docs3 <- Corpus(VectorSource(text3))
dtm3 <- TermDocumentMatrix(docs3)
m3 <- as.matrix(dtm3)
v3 <- sort(rowSums(m3),decreasing=TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
# Make cloud
set.seed(1234)
wordcloud(words = d3$word, freq = d3$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

## restaurant top 4
b4<-filter(review, business_id == top_review[4,1])
text4<-unlist(b4$text)
# Remove stop-words
text4 = removeWords(text4, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text4, file = "text4.Rdata")
text4 <- readRDS('text4.Rdata')
# take a look at the top 10 frequent words. 
sort(table(text4), decreasing=T)[1:10]
text4 = text4[c(1:1000)]
docs4 <- Corpus(VectorSource(text4))
dtm4 <- TermDocumentMatrix(docs4)
m4 <- as.matrix(dtm4)
v4 <- sort(rowSums(m4),decreasing=TRUE)
d4 <- data.frame(word = names(v4),freq=v4)
# Make cloud
set.seed(1234)
wordcloud(words = d4$word, freq = d4$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


## restaurant top 5
b5<-filter(review, business_id == top_review[5,1])
text5<-unlist(b5$text)
# Remove stop-words
text5 = removeWords(text5, c(stopwords("english"),"end","get","say","years","come","going","2012"))
saveRDS(text5, file = "text5.Rdata")
text5 <- readRDS('text5.Rdata')
# take a look at the top 10 frequent words. 
t5<-sort(table(text5), decreasing=T)[1:10]
text5 = text5[c(1:1000)]
docs5 <- Corpus(VectorSource(text5))
dtm5 <- TermDocumentMatrix(docs5)
m5 <- as.matrix(dtm5)
v5 <- sort(rowSums(m5),decreasing=TRUE)
d5 <- data.frame(word = names(v5),freq=v5)
# Make cloud
set.seed(1234)
wordcloud(words = d5$word, freq = d5$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```